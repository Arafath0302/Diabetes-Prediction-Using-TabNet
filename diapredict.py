
"""DiaPredict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18kqT51KVZPIdFmmOSLdrJBanQb23aEfu
"""

!pip -q install shap pytorch-tabnet imbalanced-learn joblib lightgbm


import numpy as np
import pandas as pd
import joblib, shap, torch
from google.colab import files

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from imblearn.over_sampling import SMOTE
from lightgbm import LGBMClassifier
from pytorch_tabnet.tab_model import TabNetClassifier

SEED = 42
np.random.seed(SEED)


import matplotlib.pyplot as plt


df = pd.read_csv('/content/drive/MyDrive/DiaHealth_Diabetes Dataset.csv')
display(df.head())

assert 'diabetic' in df.columns, "Expected a 'diabetic' column with Yes/No"

df['diabetic'] = df['diabetic'].fillna('No')

print("Unique values in 'diabetic' column after filling NaNs:", df['diabetic'].unique())


X = df.drop(columns=['diabetic'])
y = df['diabetic']

cat_cols = ['gender']
num_cols = [c for c in X.columns if c not in cat_cols]  

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=SEED
)

raw_columns = list(X.columns) 

print("Features shape:", X.shape)
print("Target shape:", y.shape)
print("Target sample:", y.unique())

cat_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('ohe', OneHotEncoder(handle_unknown='ignore'))
])
num_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer([
    ('cat', cat_pipe, cat_cols),
    ('num', num_pipe, num_cols)
])

X_train_t = preprocessor.fit_transform(X_train)
X_test_t  = preprocessor.transform(X_test)

feat_names = preprocessor.get_feature_names_out()

sm = SMOTE(random_state=SEED)
X_train_bal, y_train_bal = sm.fit_resample(X_train_t, y_train)

lgb = LGBMClassifier(random_state=SEED)
lgb.fit(X_train_bal, y_train_bal)

explainer = shap.TreeExplainer(lgb)

sv = explainer.shap_values(X_train_t)
sv = sv[1] if isinstance(sv, list) else sv
mean_abs_shap = np.abs(sv).mean(axis=0)

imp = pd.DataFrame({'feature': feat_names, 'importance': mean_abs_shap})
imp = imp.sort_values('importance', ascending=False).reset_index(drop=True)

TOP_K = 12  
selected_features = imp.head(TOP_K)['feature'].tolist()
print("Selected features:", selected_features)

name_to_idx = {n:i for i,n in enumerate(feat_names)}
sel_idx = [name_to_idx[f] for f in selected_features]

X_train_sel = X_train_t[:, sel_idx]
X_test_sel  = X_test_t[:, sel_idx]

sm_final = SMOTE(random_state=SEED)
X_train_final, y_train_final = sm_final.fit_resample(X_train_sel, y_train)

tabnet = TabNetClassifier(seed=SEED, verbose=1)
tabnet.fit(
    X_train_final.astype(np.float32), y_train_final.values.astype(int),
    eval_set=[(X_test_sel.astype(np.float32), y_test.values.astype(int))],
    eval_metric=['accuracy'],
    max_epochs=100, patience=20, batch_size=512, virtual_batch_size=128
)


y_pred = tabnet.predict(X_test_sel.astype(np.float32))
print("Accuracy :", round(accuracy_score(y_test, y_pred), 4))
print("Precision:", round(precision_score(y_test, y_pred), 4))
print("Recall   :", round(recall_score(y_test, y_pred), 4))
print("F1       :", round(f1_score(y_test, y_pred), 4))

bundle = {
    "raw_columns": raw_columns,   
    "preprocessor": preprocessor,   
    "selected_idx": sel_idx,        
    "selected_features": selected_features,
    "tabnet": tabnet                
}

joblib.dump(bundle, "model.pkl")
files.download("model.pkl")