# -*- coding: utf-8 -*-
"""DiaPredict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18kqT51KVZPIdFmmOSLdrJBanQb23aEfu
"""

!pip -q install shap pytorch-tabnet imbalanced-learn joblib lightgbm


import numpy as np
import pandas as pd
import joblib, shap, torch
from google.colab import files

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from imblearn.over_sampling import SMOTE
from lightgbm import LGBMClassifier
from pytorch_tabnet.tab_model import TabNetClassifier

SEED = 42
np.random.seed(SEED)

# Commented out IPython magic to ensure Python compatibility.

import matplotlib.pyplot as plt
# %matplotlib inline

df = pd.read_csv('/content/drive/MyDrive/DiaHealth_Diabetes Dataset.csv')
display(df.head())

# Target encode
assert 'diabetic' in df.columns, "Expected a 'diabetic' column with Yes/No"
# Fill missing values in 'diabetic' column with 'No' before mapping
df['diabetic'] = df['diabetic'].fillna('No')

# Print unique values to diagnose
print("Unique values in 'diabetic' column after filling NaNs:", df['diabetic'].unique())

# The 'diabetic' column seems to be already mapped to 0 and 1.
# Removing the mapping and astype(int) call.

X = df.drop(columns=['diabetic'])
y = df['diabetic']

# Column groups (keep simple)
cat_cols = ['gender']
num_cols = [c for c in X.columns if c not in cat_cols]  # includes numeric + 0/1 flags

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=SEED
)

raw_columns = list(X.columns)  # to enforce order later

print("Features shape:", X.shape)
print("Target shape:", y.shape)
print("Target sample:", y.unique())

# Simple imputers + encoders (beginner-friendly)
cat_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('ohe', OneHotEncoder(handle_unknown='ignore'))
])
num_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer([
    ('cat', cat_pipe, cat_cols),
    ('num', num_pipe, num_cols)
])

# Fit on train only, transform both
X_train_t = preprocessor.fit_transform(X_train)
X_test_t  = preprocessor.transform(X_test)

# Feature names for post-transform
feat_names = preprocessor.get_feature_names_out()

# Balance the TRAIN set before baseline training (no leakage into test)
sm = SMOTE(random_state=SEED)
X_train_bal, y_train_bal = sm.fit_resample(X_train_t, y_train)

# Simple baseline model for SHAP
lgb = LGBMClassifier(random_state=SEED)
lgb.fit(X_train_bal, y_train_bal)

# SHAP importance (use train to keep it simple)
explainer = shap.TreeExplainer(lgb)
# shap_values can be [class0, class1] for binary â€” take class1
sv = explainer.shap_values(X_train_t)
sv = sv[1] if isinstance(sv, list) else sv
mean_abs_shap = np.abs(sv).mean(axis=0)

imp = pd.DataFrame({'feature': feat_names, 'importance': mean_abs_shap})
imp = imp.sort_values('importance', ascending=False).reset_index(drop=True)

TOP_K = 12   # keep it simple & small
selected_features = imp.head(TOP_K)['feature'].tolist()
print("Selected features:", selected_features)

# Map to indices
name_to_idx = {n:i for i,n in enumerate(feat_names)}
sel_idx = [name_to_idx[f] for f in selected_features]

# Keep only selected columns
X_train_sel = X_train_t[:, sel_idx]
X_test_sel  = X_test_t[:, sel_idx]

# Balance TRAIN again for final model
sm_final = SMOTE(random_state=SEED)
X_train_final, y_train_final = sm_final.fit_resample(X_train_sel, y_train)

# Default TabNet (simple)
tabnet = TabNetClassifier(seed=SEED, verbose=1)
tabnet.fit(
    X_train_final.astype(np.float32), y_train_final.values.astype(int),
    eval_set=[(X_test_sel.astype(np.float32), y_test.values.astype(int))],
    eval_metric=['accuracy'],
    max_epochs=100, patience=20, batch_size=512, virtual_batch_size=128
)

# Metrics
y_pred = tabnet.predict(X_test_sel.astype(np.float32))
print("Accuracy :", round(accuracy_score(y_test, y_pred), 4))
print("Precision:", round(precision_score(y_test, y_pred), 4))
print("Recall   :", round(recall_score(y_test, y_pred), 4))
print("F1       :", round(f1_score(y_test, y_pred), 4))

bundle = {
    "raw_columns": raw_columns,     # enforce input order in the app
    "preprocessor": preprocessor,   # imputers + OHE + scaler
    "selected_idx": sel_idx,        # which transformed columns to keep
    "selected_features": selected_features,
    "tabnet": tabnet                # trained final model
}

joblib.dump(bundle, "model.pkl")
files.download("model.pkl")